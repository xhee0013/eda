---
title: "ETC5521: Exploratory Data Analysis"
subtitle: "Sculpting data using models, checking assumptions, co-dependency and performing diagnostics"
author: "Emi Tanaka"
email: "ETC5521.Clayton-x@monash.edu"
date: "Week 8 - Session 1"
color_theme: "yellow"
bgimg: "images/steve-johnson--auJqHRvM_k-unsplash.jpg"
output:
  xaringan::moon_reader:
    css:
      - ninjutsu 
      - "assets/animate.css"
      - "assets/fira-code.css"
      - "assets/boxes.css"
      - "assets/styles.css"
      - "assets/custom.css"
      - "assets/monash-logo.css"
      - "assets/monash-brand.css"
      - "assets/monash-fonts.css"
      - "assets/table.css"
      - "assets/panelset.css"
      - "assets/emi.css"
    self_contained: false 
    seal: false 
    chakra: 'libs/remark-latest.min.js'
    lib_dir: libs
    includes:
      in_header: "assets/custom.html"
    mathjax: "assets/mathjax-local/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
    nature:
      highlightStyle: github
      highlightLanguage: r 
      highlightLines: true
      highlightSpans: false 
      countIncrementalSlides: false
      slideNumberFormat: '%current%/%total%'
      navigation:
        scroll: false 
        touch: true
        click: false
      ratio: '16:9'
---

```{r, include = FALSE}
current_file <- knitr::current_input()
```
```{r, include = FALSE, eval = F}
input <- fs::path_ext_set(current_file, "html")
pagedown::chrome_print(input = input, format = "pdf", wait = 20)
```

```{r, include = FALSE}
library(tidyverse)
library(colorspace)
library(patchwork)
library(broom)
options(width = 200)
knitr::opts_chunk$set(
  fig.path = "images/week8A/",
  fig.width = 6,
  fig.height = 6,
  fig.align = "center",
  dev.args = list(bg = 'transparent'),
  #out.width = "100%",
  fig.retina = 3,
  echo = FALSE,
  warning = FALSE,
  message = FALSE,
  cache = TRUE,
  cache.path = "cache/week8/"
)
theme_set(ggthemes::theme_gdocs(base_size = 18) +
            theme(plot.background = element_rect(fill = 'transparent', colour = NA), axis.line.y = element_line(color = "black", linetype = "solid"),
                  plot.title.position = "plot",
                  plot.title = element_text(size = 24),
                  panel.background  = element_rect(fill = 'transparent', colour = NA),
                  legend.background = element_rect(fill = 'transparent', colour = NA),
                  legend.key        = element_rect(fill = 'transparent', colour = NA)
                  ) )
```

```{r titleslide, child="components/titleslide.Rmd"}
```

```{css, echo = FALSE}
.gray80 {
  color: #505050!important;
  font-weight: 300;
}
.bg-gray80 {
  background-color: #DCDCDC!important;
}
```

---

class: transition middle

# Parametric regression

---



# Parametric regression

.grid[
.item.border-right[
* .monash-blue[**Parametric**] means that the researcher or analyst assumes in advance that the data fits some type of distribution (e.g. the normal distribution). E.g. one may assume that
$$\color{blue}{y_i} = \color{red}{\beta_0} + \color{red}{\beta_1} \color{blue}{x_i} + \color{red}{\beta_2} \color{blue}{x_i^2} + \epsilon_i,$$
where $\epsilon_i \sim N(0, \color{red}{\sigma^2})$.

  * $\color{red}{red} = \text{estimated}$
  * $\color{blue}{blue} = \text{observed}$



]
.item[

## Example

```{r quad-good-fit, fig.height = 3, fig.width = 6}
set.seed(1)
tibble(id = 1:200) %>% 
  mutate(x = runif(n(), -10, 10),
         y = x^2 + rnorm(n(), 0, 5)) %>% 
  ggplot(aes(x, y)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE,
              formula = y ~ poly(x, 2),
              size = 2, color = "red")
```



]
]

---

count: false

# Parametric regression

.grid[
.item.border-right[
* .monash-blue[**Parametric**] means that the researcher or analyst assumes in advance that the data fits some type of distribution (e.g. the normal distribution). E.g. one may assume that
$$\color{blue}{y_i} = \color{red}{\beta_0} + \color{red}{\beta_1} \color{blue}{x_i} + \color{red}{\beta_2} \color{blue}{x_i^2} + \epsilon_i,$$
where $\epsilon_i \sim N(0, \color{red}{\sigma^2})$.

  * $\color{red}{red} = \text{estimated}$
  * $\color{blue}{blue} = \text{observed}$
* Because some type of distribution is assumed in advance, parametric fitting can lead to fitting a smooth curve that misrepresents the data.

]
.item[

## Examples

```{r quad-good-fit, fig.height = 3, fig.width = 6}
```
Still assuming a quadratic fit:

```{r quad-bad-fit, fig.height = 3, fig.width = 6}
tibble(id = 1:200) %>% 
  mutate(x = runif(n(), -10, 10),
         y = x^3 + rnorm(n(), 0, 5)) %>% 
  ggplot(aes(x, y)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE,
              formula = y ~ poly(x, 2),
              size = 2, color = "red")
```


]
]



---

# Simulating data from parametric models

.grid[
.item.border-right[
* In ETC5512 Wild-Caught Data, we talked about [generating data from a simple linear model](https://wcd.numbat.space/slides/week08-pisa#38).
* If a model is say:
$$y = x^2 + e, \qquad e \sim N(0, 2^2)$$
we can simulate say $200$ observations from this model for $x\in(-10,10)$ by code as shown on the right. 
```{r sim-quad, include = FALSE}
set.seed(1)
df <- tibble(id = 1:200) %>% 
        mutate(x = runif(n(), -10, 10),
               y = x^2 + rnorm(n(), 0, 2))
df
```
```{r sim-plot, fig.height = 3, fig.width = 4}
ggplot(df, aes(x, y)) +
  geom_point()
```

]
.item[
```{r sim-quad, echo = TRUE}
```
]
]

---

class: transition middle

# Logistic regression

---

# Logistic regression

* Not all parametric models assume Normally distributed errors.
* Logistic regression models the relationship between a set of explanatory variables $(x_{i1}, ..., x_{ik})$ and a set of .monash-blue[**binary outcomes**] $Y_i$ for $i = 1, ..., r$.
* We assume that $Y_i \sim B(n_i, p_i)$ and the model is given by 

$$\text{logit}(p_i) = \text{ln}\left(\dfrac{p_i}{1 - p_i}\right) = \beta_0 + \beta_1x_{i1} + ... + \beta_k x_{ik}.$$
* The function $f(p) = \text{ln}\left(\dfrac{p}{1 - p}\right)$ is called the .monash-blue[**logit**] function, continuous with range $(-\infty, \infty)$, and if $p$ is the probablity of an event, $f(p)$ is the log of the odds.

---

# .orange[Case study] .circle.bg-orange[1] Menarche

In 1965, the average age of 25 homogeneous groups of girls was
recorded along with the number of girls who have reached
menarche out of the total in each group.

.panelset[
.panel[.panel-name[ðŸ“Š]
```{r menarche-data, include = FALSE}
data(menarche, package = "MASS")
skimr::skim(menarche)
```
```{r menarche-plot, fig.height = 4, fig.width = 5}
ggplot(menarche, aes(Age, Menarche/Total)) + 
  geom_point() +
  geom_smooth(method = "glm",
              formula = y ~ x,
              se = FALSE,
              method.args = list(family = "binomial"))
```


]
.panel[.panel-name[data]
.h200.scroll-sign[
```{r menarche-data, echo = TRUE, render = knitr::normal_print}
```

]]
.panel[.panel-name[R]
```{r menarche-plot, echo = TRUE}
```
]

]

.footnote[
Milicer, H. and Szczotka, F. (1966) Age at Menarche in Warsaw girls in 1965. Human Biology 38, 199â€“203.
]

---

# Simulating data from logistic regression 

.grid[
.item.border-right[


```{r logit, echo = TRUE}
fit1 <- glm(Menarche/Total ~ Age, 
            family = "binomial", 
            data = menarche)
(beta <- coef(fit1))
```

* The fitted regression model is given as:
$$\text{logit}(\hat{p}_i) = \hat{\beta}_0  + \hat{\beta}_1 x_{i1}.$$
* Taking the exponential of both sides and rearranging we get
$$\hat{p}_i = \dfrac{1}{1 + e^{-(\hat{\beta}_0  + \hat{\beta}_1 x_{i1})}}.$$
]
.item[
```{r sim-logistic, echo = TRUE}
menarche %>% 
  rowwise() %>% # simulating from first principles
  mutate(
    phat = 1/(1 + exp(-(beta[1] + beta[2] * Age))),
    simMenarche = rbinom(1, Total, phat)) #<<
```

]

]

--

<div class="bg-white" style="position:absolute;bottom:10px;right:10px;width:40%;padding:20px;border:2px solid black;">
<i class="fas fa-sticky-note monash-blue"></i> If simulating data from a model object, <code>simulate</code> function usually can do this for you!
</div>



---

# Diagnostics for logistic regression models

.grid[
.item.border-right[

* One diagnostic is to compare the observed and expected proportions under the logistic regression fit.

```{r fit-logistic, echo = TRUE}
df1 <- menarche %>% 
  mutate(
    pexp = 1/(1 + exp(-(beta[1] + beta[2] * Age))),
    pobs = Menarche / Total)
```
```{r plot-logistic, fig.height = 4, fig.width = 4}
ggplot(df1, aes(pobs, pexp)) +
  geom_point() + 
  geom_abline(slope = 1, intercept = 0,
              color = "red") +
  labs(x = "Observed proportion",
       y = "Expected proportion")
```

]
.item50[

{{content}}

]

]

--

* Goodness-of-fit type test is used commonly to assess the fit as well.

* E.g. Hosmerâ€“Lemeshow test, where test statistic is given as 

<span style="font-size:14pt!important;">
$$H = \sum_{i = 1}^r \left(\dfrac{(O_{1i} - E_{1g})^2}{E_{1i}} + \dfrac{(O_{0i} - E_{0g})^2}{E_{0i}}\right)$$
where $O_{1i}$ $(E_{1i})$ and $O_{0i}$ $(E_{0i})$ are observed (expected) frequencies for successful and non-successful events for group $i$, respectively.
</span>

```{r HLtest, echo = TRUE}
vcdExtra::HLtest(fit1)
```

---

class: transition middle

# Diagnostics for linear models

---

# Assumptions for linear models

.grid[
.item.border-right[
For $i \in \{1, ..., n\}$,

$$Y_i = \beta_0 + \beta_1x_{i1} + ... + \beta_{k}x_{ik} + \epsilon_i,$$
where $\epsilon_i \sim NID(0, \sigma^2)$ or in matrix format,

$$\boldsymbol{Y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}, \quad \boldsymbol{\epsilon} \sim N(\boldsymbol{0}, \sigma^2 \mathbf{I}_n)$$

<div style="font-size:12pt;padding-left:20px;">
where 

<ul>
<li> $\boldsymbol{Y} = (Y_1, ..., Y_n)^\top$,</li>
<li> $\boldsymbol{\beta} = (\beta_0, ..., \beta_k)^\top$, </li>
<li> $\boldsymbol{\epsilon} = (\epsilon_1, ..., \epsilon_n)^\top$, and </li>
<li> $\mathbf{X} = \begin{bmatrix}\boldsymbol{1}_n & \boldsymbol{x}_1 & ... & \boldsymbol{x}_k \end{bmatrix}$, where </li>
<li> $\boldsymbol{x}_j =(x_{1j}, ..., x_{nj})^\top$ for $j \in \{1, ..., k\}$</li>
</ul>
</div>


]
.item[

]

]

---

count: false

# Assumptions for linear models

.grid[
.item.border-right[
For $i \in \{1, ..., n\}$,

$$Y_i = \beta_0 + \beta_1x_{i1} + ... + \beta_{k}x_{ik} + \epsilon_i,$$
where $\color{red}{\epsilon_i \sim NID(0, \sigma^2)}$ or in matrix format,

$$\boldsymbol{Y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}, \quad \color{red}{\boldsymbol{\epsilon} \sim N(\boldsymbol{0}, \sigma^2 \mathbf{I}_n)}$$

<div style="font-size:12pt;padding-left:20px;">
where 

<ul>
<li> $\boldsymbol{Y} = (Y_1, ..., Y_n)^\top$,</li>
<li> $\boldsymbol{\beta} = (\beta_0, ..., \beta_k)^\top$, </li>
<li> $\boldsymbol{\epsilon} = (\epsilon_1, ..., \epsilon_n)^\top$, and </li>
<li> $\mathbf{X} = \begin{bmatrix}\boldsymbol{1}_n & \boldsymbol{x}_1 & ... & \boldsymbol{x}_k \end{bmatrix}$, where </li>
<li> $\boldsymbol{x}_j =(x_{1j}, ..., x_{nj})^\top$ for $j \in \{1, ..., k\}$</li>
</ul>
</div>


]
.item[
This means that we assume

1. $E(\epsilon_i) = 0$ for $i \in \{1, ..., n\}.$
2. $\epsilon_1, ..., \epsilon_n$ are independent.
3. $Var(\epsilon_i) = \sigma^2$ for $i \in \{1, ..., n\}$ (i.e. homogeneity).
4. $\epsilon_1, ..., \epsilon_n$ are normally distributed.

{{content}}

]

]

--

<br><br>
<i>So how do we check it?</i>




---

# Model diagnostics for linear models

<div class="grid" style="grid-template: 1fr 1fr / 1fr 1fr">

.item[
Plot $Y_i$ vs $x_i$ to see if there is $\approx$ a linear relationship between $Y$ and $x$.

```{r, echo=F, fig.height=3, fig.width=3, fig.align="center"}
dat <- read_csv(here::here("data/sleep.csv"))
dat %>%
  ggplot(aes(log(BodyWt), log(BrainWt))) + geom_point() +
  geom_smooth(method="lm", se=F) +
  labs(x="log(Body) (kg)", y="log(Brain) (g)") 
```

]
.item.bg-gray80[

A boxplot of the residuals $R_i$ to check for symmetry.
```{r, echo=F, fig.height=2.5, fig.width=2.5, fig.align="center", dev='svg', dev.args=list(bg = "transparent")}
lm(log(BrainWt) ~ log(BodyWt), data=dat) %>%
  augment() %>% 
  ggplot(aes(1,.std.resid)) + 
  geom_boxplot() + 
  labs(x="", y="Residual") + 
  theme(axis.text.x = element_blank())
```

]
.item.bg-gray80[
To check the homoscedasticity assumption, plot $R_i$ vs $x_i$. There should be no obvious patterns.

```{r, echo=F, fig.height=2.2, fig.width=2.2, fig.align="center", dev='svg', dev.args=list(bg = "transparent")}
lm(log(BrainWt) ~ log(BodyWt), data=dat) %>%
  augment() %>% 
  ggplot(aes(`log(BodyWt)`, .std.resid)) + 
  geom_point() + 
  labs(x="log(Body) (kg)", y="Residual") +
  geom_hline(yintercept=0, color="blue") 
```
]
.item[
A normal Q-Q plot, i.e. a plot of the ordered residuals vs $\Phi^{-1}(\frac{i}{n+1})$.

```{r, echo=F, fig.height=2.2, fig.width=2.2, fig.align="center", dev='svg', dev.args=list(bg = "transparent")}
lm(log(BrainWt) ~ log(BodyWt), data=dat) %>%
  augment() %>% 
  ggplot(aes(sample=.std.resid)) + 
  stat_qq() + stat_qq_line(color="blue")
```

]

</div>




---

# Assessing (A1) $E(\epsilon_i) = 0$ for $i=1,\ldots,n$

* It is a property of the least squares method that $$\sum_{i=1}^n R_i = 0,\quad
\text{so}\quad \bar R_i  = 0$$ for $R_i = Y_i - \hat{Y}_i$, hence (A1)  will always appear valid "overall". 
* Trend in residual versus fitted values or covariate can indicate "local"
failure of (A1). 
* What do you conclude from the following plots?

```{r sim-plots, echo=F, fig.width=12, fig.height=3, dev='svg', dev.args=list(bg = "transparent")}
set.seed(2019)
n <- 100
x <- seq(0, 1, length.out = n)
y1 <- x + rnorm(n) / 3                 #  Linear
y2 <- 3 * (x - 0.5) ^ 2 + 
  c(rnorm(n / 2)/3, rnorm(n / 2)/6)  #  Quadratic
y3 <- - 0.25 * sin(20 * x - 0.2) + x + rnorm(n) / 3    #  Non-linear
g1 <- lm(y1 ~ x) %>% augment() %>%
  ggplot(aes(.fitted, .resid)) + 
  geom_point() + 
  labs(x="Fitted Values", y="Residual", tag="(1)") +
  geom_hline(yintercept=0, color="blue") 
g2 <- lm(y2 ~ x) %>% augment() %>%
  ggplot(aes(.fitted, .resid)) + 
  geom_point() + 
  labs(x="Fitted Values", y="Residual", tag="(2)") +
  geom_hline(yintercept=0, color="blue") 
g3 <- lm(y3 ~ x) %>% augment() %>%
  ggplot(aes(.fitted, .resid)) + 
  geom_point() +
  labs(x="Fitted Values", y="Residual", tag="(3)") +
  geom_hline(yintercept=0, color="blue") 

g1 + g2 + g3
```

---

# Assessing (A2)-(A3) 

.grid[.item.bg-gray80[

### (A2) $\epsilon_1, \ldots ,\epsilon_n$ are independent

* If (A2) is correct, then residuals should appear randomly scattered
about zero if plotted against fitted values or covariate.
* Long sequences of positive residuals followed by sequences of negative residuals in $R_i$ vs $x_i$ plot suggests that the error terms are not independent.
]
.item[

### (A3) $Var(\epsilon_i) = \sigma^2$ for $i=1,\ldots,n$

* If (A3) holds then the spread of the residuals should be roughly the same across the fitted values or covariate. 

<Br>

```{r sim-plots, fig.height = 4, fig.width = 10}
```


]
]

---

# Assessing (A4) $\epsilon_1, \ldots ,\epsilon_n$ are normally distributed

.grid[.item[
### Q-Q Plots

* The function `qqnorm(x)` produces a Q-Q plot of the ordered vector `x` against the quantiles of the normal distribution.
* The $n$ chosen normal quantiles $\Phi^{-1}(\frac{i}{n+1})$ are easy to calculate but more sophisticated ways exist:
   * $\frac{i}{n+1} \mapsto \frac{i-3/8}{n+1/4}$, default in `qqnorm`. 
   * $\frac{i}{n+1} \mapsto \frac{i-1/3}{n+1/3}$, recommended by Hyndman and Fan (1996).
<!-- * Symmetry, heavy or light tails, location and spread can be "easily" seen in Q-Q plots. How? -->

]
.item.bg-gray80[

### In R

```{r, echo = T, eval = F}
fit <- lm(y ~ x)
```


By "hand"
```{r, eval=F, echo = T}
plot(qnorm((1:n) / (n + 1)), sort(resid(fit)))
```

By `base`

```{r, eval=F, echo = T}
qqnorm(resid(fit))
qqline(resid(fit))
```

By `ggplot2`

```{r, eval=F, echo = T}
data.frame(residual = resid(fit)) %>% 
  ggplot(aes(sample = residual)) + 
  stat_qq() + stat_qq_line(color="blue")
```


]
]

.footnote[
Reference: Hyndman and Fan (1996). *Sample quantiles in statistical packages*, American Statistician, 50, 361--365.
]

---

# Examining the simulated data further

<div class="grid" style="grid-template: 1fr 1fr / 1fr 1fr">

.item[
```{r, echo=F, fig.height=4, fig.width=8, fig.align="center"}
dat3sim <- data.frame(y=c(y1, y2, y3), x=rep(x, times=3), Simulation=rep(1:3, each=length(x)))
dat3sim %>%
  ggplot(aes(x,y)) + geom_point() +
  geom_smooth(method="lm", se=F) + facet_grid( . ~ Simulation) +
  theme(axis.text = element_blank())
```

]
.item.bg-gray80[

```{r, echo=F, fig.height=4, fig.width=8, fig.align="center",y2dev='svg', dev.args=list(bg = "transparent")}
M1 <- lm(y1 ~ x); M2 <- lm(y2 ~ x); M3 <- lm(y3 ~ x)
data.frame(residual=c(resid(M1), resid(M2), resid(M3)),
           Simulation=rep(1:3, each=length(x))) %>%
  ggplot(aes(x=factor(Simulation), y=residual)) + geom_boxplot() + labs(x="", y="Residual")
```



]
.item.bg-gray80[

{{content}}

]
.item[
```{r, echo=F, fig.height=4, fig.width=8, fig.align="center",y2dev='svg', dev.args=list(bg = "transparent")}
data.frame(residual=c(resid(M1), resid(M2), resid(M3)),
           Simulation=rep(1:3, each=length(x))) %>%
  ggplot(aes(sample=residual)) + stat_qq() + 
  stat_qq_line(color="blue") +
  facet_grid( . ~ Simulation) 
```

]

</div>

--

Simulation scheme
```{r, echo = TRUE}
n <- 100
x <- seq(0, 1, length.out = n)
y1 <- x + rnorm(n) / 3                  #  Linear
y2 <- 3 * (x - 0.5) ^ 2 + 
  c(rnorm(n / 2)/3, rnorm(n / 2)/6)     #  Quadratic
y3 <- -0.25 * sin(20 * x - 0.2) + 
  x + rnorm(n) / 3                      #  Non-linear

M1 <- lm(y1 ~ x); M2 <- lm(y2 ~ x); M3 <- lm(y3 ~ x)

```

---

# Revisiting outliers

* We defined [outliers in week 4](https://eda.numbat.space/lectures/lecture-04a#9) as "observations that are significantly different from the majority" when studying univariate variables.
* There is actually no hard and fast definition. <br><br>

.info-box[
We can also define an outlier as a data point that emanates from a different model than do the rest of the data.
]

<br>

* Notice that this makes this definition *dependent on the model* in question.

---

class: transition middle

# Pop Quiz

Would you consider the yellow points below as outliers?

```{r, echo=F, fig.height=5, fig.width=10, fig.align="center"}
n <- 20
set.seed(1)
shifty <- rep(0, n); shifty[5] <- 10
g1 <- data.frame(x=seq(1, 20, 1)) %>% 
    mutate(y=x + rnorm(n, 0, 1)) %>% 
  ggplot(aes(x, y + shifty, color=factor(shifty))) + geom_point(size=6) + 
  labs(x="x", y="y", tag="(A)") +
  guides(color=F) +
  scale_color_manual(values = c("black", "yellow"))

n <- 20
set.seed(2)
g2 <- data.frame(x=c(seq(1, 19, 1), 30)) %>% 
    mutate(y=x + rnorm(n, 0, 1)) %>% 
  ggplot(aes(x, y, color=factor(c(rep(0,19), 1)))) + geom_point(size=6) + 
  labs(x="x", y="y", tag="(B)") +
  guides(color=F)+
  scale_color_manual(values = c("black", "yellow"))

g1 + g2 
```

---

# Outlying values 


.grid[
.item.border-right[
* As with simple linear regression the fitted model should not be used to predict $Y$ values for $\boldsymbol{x}$ combinations that are well away from the set of observed $\boldsymbol{x}_i$ values. 
* This is not always easy to detect!

```{r, fig.height = 4}
tibble(id = 1:20) %>% 
  mutate(x1 = runif(n()),
         x2 = 1 - 4 * x1 + x1^2 + rnorm(n(), 0, 0.1)) %>% 
  add_row(x1 = 0.6, x2 = 0.6) %>% 
  ggplot(aes(x1, x2)) +
  geom_point(size = 4) + 
  annotate("text", x = 0.55, y = 0.55, label = "P", 
           size = 10)
```


]
.item[

* Here, a point labelled P has $x_1$ and $x_2$ coordinates well within their respective ranges but P is not close to the observed sample values in 2-dimensional space. 

* In higher dimensions this type of behaviour is even harder to detect but we need to be on guard against extrapolating to extreme values. 


]
]

---

# Leverage 

* The matrix $\mathbf{H} = \mathbf{X}(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top$ is referred to as the .monash-blue[**hat matrix**].
* The $i$-th diagonal element of $\mathbf{H}$, $h_{ii}$, is called the .monash-blue[**leverage**] of the $i$-th observation.
* Leverages are always between zero and one,
$$0 \geq h_{ii} \geq 1.$$
* Notice that leverages are not dependent on the response!
* Points with high leverage can exert a lot of influence on the parameter estimates

---

# Studentized residuals

In order to obtain residuals with equal variance, many texts recommend using the .monash-blue[**studentised residuals**]
$$R_i^* = \dfrac{R_i} {\hat{\sigma} \sqrt{1 - h_{ii}}}$$
for diagnostic checks.

---

# Cook's distance

* .brand-blue[Cook's distance], $D$, is another measure of influence: 
\begin{eqnarray*}
D_i &=& \dfrac{(\hat{\boldsymbol{\beta}}- \hat{\boldsymbol{\beta}}_{[-i]})^\top Var(\hat{\boldsymbol{\beta}})^{-1}(\hat{\boldsymbol{\beta}}- \hat{\boldsymbol{\beta}}_{[-i]})}{p}\\
&=&\frac{R_i^2 h_{ii}}{(1-h_{ii})^2p\hat\sigma^2},
\end{eqnarray*}
where $p$ is the number of elements in $\boldsymbol{\beta}$, $\hat{\boldsymbol{\beta}}_{[-i]}$ and $\hat Y_{j[-i]}$ are least squares estimates and the fitted value obtained by fitting the model ignoring the $i$-th data point $(\boldsymbol{x}_i,Y_i)$, respectively.

---


# .orange[Case study] .circle.bg-orange[2] Social media marketing

Data collected from advertising experiment to study the impact of three advertising medias (youtube, facebook and newspaper) on sales.


.panelset[
.panel[.panel-name[ðŸ“Š]



```{r marketing-data, include = FALSE}
data(marketing, package="datarium")
skimr::skim(marketing)
```
```{r marketing-plot, fig.height = 6, fig.width = 7}
GGally::ggpairs(marketing, progress=F)
```


]
.panel[.panel-name[data]
.h200.scroll-sign[
```{r marketing-data, echo = TRUE, render = knitr::normal_print}
```

]]
.panel[.panel-name[R]
```{r marketing-plot, echo = TRUE, eval = FALSE}
```

]

]





---

# Extracting values from models in R

* The leverage value, studentised residual and Cook's distance can be easily extracted from a model object using `broom::augment`.
  * `.hat` is the leverage value
  * `.std.resid` is the studentised residual
  * `.cooksd` is the Cook's distance

```{r, echo = TRUE}
fit <- lm(sales ~ youtube * facebook, data = marketing)
broom::augment(fit)

```

---

# Resources and Acknowledgement

- Some of these slides were inspired by STAT3012 Applied Linear Models at The University of Sydney by Prof Samuel Muller
- Cook & Weisberg (1994) 
"An Introduction to Regression Graphics"
- Data coding using [`tidyverse` suite of R packages](https://www.tidyverse.org) 
- Slides constructed with [`xaringan`](https://github.com/yihui/xaringan), [remark.js](https://remarkjs.com), [`knitr`](http://yihui.name/knitr), and [R Markdown](https://rmarkdown.rstudio.com).

---

```{r endslide, child="components/endslide.Rmd"}
```
