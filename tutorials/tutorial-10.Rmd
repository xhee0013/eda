---
title: "ETC5521 Tutorial 10"
subtitle: "Exploring data having a space and time context"
author: "Dr Di Cook"
date: "Week 10"
output:
  bookdown::html_document2:
    toc: true
    number_sections: false
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  echo = FALSE, 
  message = FALSE, 
  warning = FALSE, 
  cache = TRUE,
  eval = FALSE,
  cache.path = "cache/",
  fig.path = "images/tutorial10/",
  fig.align = "center"
)
```

```{r libraries}
library(tidyverse)
library(here)
library(tsibble)
library(brolgar)
library(lubridate)
library(DAAG)
library(broom)
library(patchwork)
library(colorspace)
library(GGally)
library(tsibbledata)
library(forcats)
```

## `r emo::ji("warning")` Tutorial will be recorded

This tutorial, including the chat, will be recorded, so that you can re-visit the instructions later, as needed. This includes private chats. It is expected that you are respectful of your class members and tutors, while actively engaging in the work. 

## `r emo::ji("target")` Objectives

These exercise are to do some exploratory analysis with graphics and statistical models, focussing on temporal data analysis.  

## `r emo::ji("wrench")` Preparation 

```{r pkgs, echo=TRUE, eval = FALSE}
install.packages(c("tidyverse", "here", "DAAG", "chron", "broom", "patchwork", "colorspace", "tsibble", "lubridate", "tsibbledata", "forcats"))
remotes::install_github("ggobi/GGally")
remotes::install_github("njtierney/brolgar")
remotes::install_github("hrbrmstr/streamgraph")
```

**GRAB A COPY OF THE .Rmd FILE TO GET HELPFUL CODE FOR STARTING ON THE PROBLEMS** If you see `???` in the code this is a place where you will need to fill in something!

## Exercise 0: Introduction

In the chat window, say hello, and if you feel comfortable tell us something fun about yourself, or what you have done this last week.

## Exercise 1: Australian rain 

This exercise is based on one from Unwin (2015), and uses the `bomregions` data from the `DAAG` package. The data contains regional rainfall for the years 1900-2008. The regional rainfall numbers are area-weighted averages for the respective regions. Extract just the rainfall columns from the data, along with year.

a. What do you think area-weighted averages are, and how would these be calculated?
b. Make line plots of the rainfall for each of the 7 regions and the Australian averages. What do you learn about rainfall patterns  across the years and regions? (Hint: you can make these with the `ggduo` function in the `GGally` package.)
c. It can be difficult to assess correlation between multiple series using line plots, and the best way to check correlation between multiple series is to make a scatterplot. Make a splom for this data, ignoring year. What regions have strong positive correlation between their rainfall averages?
d. One of the consequences of climate change for Australia is that some regions are likely getting drier. Make a transformation of the data to compute the difference between rainfall average in the year, and the mean over all years. Using a bar for each year, make a barchart that examines the differences in the yearly rainfall over time. (Hint: you will need to pivot the data into tidy long form to make this easier.) Are there some regions who have negative differences in recent years? What else do you notice?

```{r}
data(bomregions)
ggduo(bomregions, "Year", c("eastRain", "seRain", "southRain", "swRain", "westRain", "northRain", "mdbRain", "auRain"))
```

```{r}
ggpairs(bomregions[, c(10:17)])
```

```{r}
med_ctr <- function(x, na.rm = TRUE) {
  x-mean(x, na.rm = TRUE)
}
bomrain <- bomregions %>% as_tibble() %>%
  select(Year, contains("Rain")) %>%
  mutate_at(c("eastRain", "seRain","southRain", 
              "swRain", "westRain", "northRain", 
              "mdbRain", "auRain"), med_ctr) %>%
  pivot_longer(eastRain:auRain, names_to = "area", 
               values_to = "rain")
ggplot(bomrain, aes(x=Year, y=rain)) +
  geom_col() +
  facet_wrap(~area, ncol=1, scales="free_y")
```

## Exercise 2: US unemployment 

This exercise is based on and example in Oscar Perpinan Lamigueiro (2018) "Displaying Time Series, Spatial, and Space-Time Data with R". Read the US employment data from the book web site. This contains monthly unemployment numbers from 2000 through 2012 in different sectors.

a. Transform the data into tidy long form and convert to a `tsibble`. Make a line plot coloured by sector. What do you learn about unemployment during this time frame from this chart? 
b. We are going to re-arrange the data now to examine the monthly patterns by year and sector. Create new variables for month and year from the date variable. Now make a line plot of count by month coloured by year, using an appropriately sequential colour palette, and facet by sector. Are there some sectors that have a seasonal pattern? Are there some sectors who were not affected by the 2008 economic downfall?
c. This next way to look at the data is like a stacked bar chart for time series. Using the same code as in question a., change `geom_line` with `geom_area`, to stack the series, with a different fill colour for each sector. What do you learn about the magnitude of the 2008 economic crisis? Can you read much from this chart about the effect on different sectors?
```{r}
US_unemp <- read_csv("https://raw.githubusercontent.com/oscarperpinan/bookvis/master/data/unemployUSA.csv") %>%
  select(!contains("Annual")) %>%
  pivot_longer(`Jan 2000`:`Dec 2012`, 
               names_to = "date", 
               values_to = "count") %>%
  mutate(date = as.Date(paste("01", date), "%d %b %Y")) %>%
  as_tsibble(index = date, key = `Series ID`)
ggplot(US_unemp, aes(x=date, y=count, colour = `Series ID`)) + geom_line()
US_unemp <- US_unemp %>%
  mutate(month = month(date, label = TRUE),
         year = year(date)) 
ggplot(US_unemp, aes(x=month, y=count, 
                     colour = year, group = year)) +
  geom_line() + 
  facet_wrap(~`Series ID`, ncol=5, scales="free_y") +
  scale_x_discrete("", labels = c("Jan"="J", "Feb"="F", 
                                  "Mar"="M", "Apr"="A", 
                                  "May"="M", "Jun"="J",
                                  "Jul"="J", "Aug"="A",
                                  "Sep"="S", "Oct"="O",
                                  "Nov"="N", "Dec"="D")) +
  scale_colour_viridis_c("", breaks = seq(2000, 2012, 4),
    guide = guide_colourbar(barwidth = 15, 
                            barheight = 1)) +
  theme_bw() +
  theme(legend.position = "bottom")
ggplot(US_unemp, aes(x=date, y=count, fill = `Series ID`)) + geom_area(position = "stack") + theme_bw() +
  scale_x_date("", date_breaks = "3 years", 
               date_labels = "%Y") +
  scale_fill_discrete("") +
  theme(legend.position = "bottom")

library(streamgraph)
US_unemp %>% 
  rename("sector" = `Series ID`) %>%
streamgraph("sector", "count", "date") 
```



## Exercise 3: Lynx trappings periodicity

This is a classic data example: Annual numbers of lynx trappings for 1821â€“1934 in Canada, from Brockwell & Davis (1991). It is a classic because it looks periodic, but it really doesn't have a period. Here we look at two ways to examine the cyclic nature to check for periodicity.

a. Create two new variables by rounding the year into a decade, and the remainder into a year in the decade.
b. Make a line plot of count by year. Add a vertical line every 10 years. If you start from 1928, the location of the first peak, you can check the  peak locations in  subsequent years. Is the peak roughly every 10 years?
c. Cut the series into decades, and make overlaid line plots of count vs year in decade, using decade as the group variable. This is like looking at seasonality, like we might look at seasonal patterns in a year by examining the months. If the series is cyclic, particularly with peaks every 10 years, the peaks should line up. Do they?

```{r}
lynx_tsb <- as_tsibble(lynx) %>%
  rename(count = value)
lynx_tsb <- lynx_tsb %>%
  mutate(decade = round(index/10, 0), 
         yr_decade = index %% 10)
ggplot(lynx_tsb, aes(x=index, y=count)) +
  geom_vline(xintercept = seq(1828, 1928, 10), colour="orange") +
  geom_line() + 
  xlab("") + ylab("count") +
  theme_bw()
```

## Exercise 4: Missing values in NYC bikes data

In [Earo Wang's blog post](https://blog.earo.me/2018/12/20/reintro-tsibble/) introducing `tsibble` she used NYC bikes data. This data is now made available in the `tsibbledata` package. 

a. Focusing on May, aggregate the data to hour, and count the number of trips in each hour. 
b. Has the data got missing values?
c. Make a line plot of the hourly trips over the month. Why does it look like there are no missings? Why is the line not broken by missings? 
d. Use `fill_gaps` to make  implicit missings explicit. Re-make the line plot from question c again. 
e. To focus on the missings, we can make a plot of the places where there are gaps, by plotting the data created by the `count_gaps` function. How extensive are the missing values?
f. Focusing on the hour when missings occur, check if there are some times of the day that missings are more frequent. 
```{r nycbikes, eval=FALSE}
# Select May
hourly_trips <- nyc_bikes %>% 
  filter(month(start_time) == 5) %>%
  index_by(start_hour = floor_date(start_time, unit = "1 hour")) %>% 
  summarise(ntrips = n()) %>% 
  as_tsibble()
ggplot(hourly_trips, aes(x=start_hour, y=ntrips)) +
  geom_line()
hourly_trips %>% fill_gaps(.full = TRUE) %>%
  ggplot(aes(x=start_hour, y=ntrips)) +
  geom_line()
nycbikes_gaps <- count_gaps(hourly_trips, .full = TRUE)
ggplot(nycbikes_gaps) +
  geom_linerange(aes(xmin = .from, xmax = .to, y=1)) +
  geom_point(aes(x = .from, y=1)) +
  geom_point(aes(x = .to, y=1)) 
scan_gaps(hourly_trips)  %>%
  mutate(time = hour(start_hour)) %>% 
  ggplot(aes(x = time)) +
  geom_bar()
```

## Exercise 5: Imputing missings for pedestrian sensor using a model

We saw in the lecture notes that imputing by simple method such as mean or moving average doesn't work well with multiple seasonality in a time series. Here we will use a linear model to capture the seasonality and produce better imputations for the pedestrian sensor data (from the `tsibble` package).

a. What are the  multiple seasons of the pedestrian sensor data, particularly the traffic at QV market, or Southern Cross Station?
b. Check and fill the gaps for the pedestrian sensor data with NA. Subset to just the QV market sensor.
c. Create a new variable to indicate if a day is a non-working day, called `hol`. Make hour a factor - this helps to make a simple model for a non-standard daily pattern.
c. Fit a linear model with Count as the response on  predictors `Time` and `hol` interacted. 
d. Predict the count for all the data at the sensor. 
e. Make a line plot focusing on the last two weeks in 2015, where there was a day of missings, where the missing counts are substituted by the model predictions. Do you think that these imputed values match the rest of the series, nicely?

```{r}
has_gaps(pedestrian, .full = TRUE)
ped_gaps <- pedestrian %>% 
  count_gaps(.full = TRUE)
ped_full <- pedestrian %>% 
  fill_gaps(.full = TRUE)
library(chron)
library(broom)
hol <- holiday_aus(2015:2016, state = "VIC")
ped_qvm <- ped_full %>% 
  filter(Sensor == "QV Market-Elizabeth St (West)") %>%
  mutate(hol = is.weekend(Date)) %>%
  mutate(hol = ifelse(Date %in% hol, TRUE, hol)) %>%
  mutate(Date = as_date(Date_Time), Time = hour(Date_Time)) %>%
  mutate(Time = factor(Time))
ped_qvm_lm <- lm(Count~Time*hol, data=ped_qvm)
ped_qvm$pCount <- predict(ped_qvm_lm, ped_qvm)
ped_qvm_sub <- ped_qvm %>%
  filter(month(Date_Time) == 12, year(Date_Time) == 2015,
         mday(Date_Time) > 21) 
ggplot(ped_qvm_sub) +   
    geom_line(aes(x=Date_Time, y=Count)) +
    geom_line(data=filter(ped_qvm_sub, is.na(Count)), 
                      aes(x=Date_Time, 
                          y=pCount), 
                      colour="orange") +
  scale_x_datetime("", date_breaks = "1 day", 
                   date_labels = "%a %d")
```



## Exercise 6: Men's heights 

The `heights` data provided in the `brolgar` package contains average male heights in 144 countries from 1500-1989. 

a. What's the time index for this data? What is the key?
b. Filter the data to keep only measurements since 1700, when there are records for many countries. Make a spaghetti plot for the values from Australia. Does it look like Australian males are getting taller?
c. Check the number of observations for each country. How many countries have less than five years of measurements? Filter these countries out of the data, because we can't study temporal trend without sufficient measurements. 
d. Make a spaghetti plot of all the data, with a smoother overlaid. Does it look like men are generally getting taller?
e. Use `facet_strata` to break the data into subsets using  the `year`, and plot is several facets. What sort of patterns are there in terms of the earliest year that a country appears in the data?
f. Compute the three number summary (min, median, max) for each country. Make density plots of these statistics, overlaid in a single plot, and a parallel coordinate plot of thee three statistics. What is the average minimum (median, maximum) height across countries? Are there some countries who have roughly the same minimum, median and maximum height?
g. Which country has the tallest men? Which country has highest median male height? Which country has the shortest men? Would you say that the distribution of heights within a  country is similar for all countries?
```{r}
heights <- brolgar::heights %>% filter(year > 1700)
heights_oz <- heights %>% 
  filter(country == "Australia") %>% 
  slice_tail(n = 5)
ggplot(heights_oz,
       aes(x = year,
           y = height_cm,
           group = country)) + 
  geom_point() + 
  geom_line()
heights <- heights %>% 
  add_n_obs() %>% 
  filter(n_obs >= 5)
ggplot(heights,
       aes(x = year,
           y = height_cm)) + 
  geom_line(aes(group = country), alpha = 0.3) + 
  geom_smooth(se=FALSE)
heights <- as_tsibble(heights,
                      index = year,
                      key = country,
                      regular = FALSE)
ggplot(heights, aes(x = year,
           y = height_cm,
           group = country)) + 
  geom_line() + 
  facet_strata(along = -year)
heights_three <- heights %>%
  features(height_cm, c(
    min = min,
    median = median,
    max = max
  ))
heights_three_l <- heights_three %>% 
  pivot_longer(cols = min:max,
               names_to = "feature",
               values_to = "value")

p1 <- heights_three_l %>% 
  ggplot(aes(x = value,
             fill = feature)) + 
  geom_density(alpha = 0.5) +
  labs(x = "Value",
       y = "Density",
       fill = "Feature") + 
  scale_fill_discrete_qualitative(palette = "Dark 3") +
  # inset the legend?
  theme(legend.position = "none",
        aspect.ratio = 1)

p2 <- heights_three_l %>% 
 ggplot(aes(x = feature,
           y = value,
           group = country)) + 
  geom_line(alpha = 0.6) +
  theme(aspect.ratio = 1)

heights_three <- heights_three %>% 
  mutate(country = factor(country)) %>%
  mutate(country = fct_reorder(country, median)) 
p3 <- heights_three %>%
    ggplot() + 
    geom_point(aes(x = country,
           y = median)) +
    geom_errorbar(aes(x = country, 
                      ymin=min, ymax=max), 
                  alpha = 0.6, width=0) +
    xlab("") + ylab("heights") +
    coord_flip()
 
design <- "
113
223
##3"
p1 + p2 + p3 + 
  plot_layout(design = design)
```
